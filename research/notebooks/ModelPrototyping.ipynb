{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import cv2\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config file\n",
    "RAW_PATH = \"../../data/raw\"\n",
    "PROCESSED_PATH = \"../../data/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils file\n",
    "import joblib\n",
    "\n",
    "def dump(value = None, filename = None):\n",
    "    if value is not None and filename is not None:\n",
    "        joblib.dump(value=value, filename=filename)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"value and filename cannot be None\".capitalize())\n",
    "    \n",
    "def load(filename):\n",
    "    if os.path.exists(filename):\n",
    "        return joblib.load(filename=filename)\n",
    "    else:\n",
    "        raise Exception(\"File not found\".capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self, image_path = None, batch_size = 4):\n",
    "        self.image_path = image_path\n",
    "        self.batch_size = batch_size\n",
    "        self.base_images = list()\n",
    "        self.mask_images = list()\n",
    "\n",
    "    def base_transforms(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((256,256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "    def mask_transforms(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((256,256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "\n",
    "    def unzip_folder(self):\n",
    "        if os.path.exists(RAW_PATH):\n",
    "            with zipfile.ZipFile(self.image_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(os.path.join(RAW_PATH, \"segmented\"))\n",
    "        else:\n",
    "            raise Exception(\"Raw data folder not found\".capitalize())\n",
    "\n",
    "    def process_segmented_data(self):\n",
    "        if os.path.join(RAW_PATH, \"segmented\"):\n",
    "            self.images_directory = os.path.join(RAW_PATH, \"segmented\")\n",
    "\n",
    "            self.base_directory = os.path.join(\n",
    "                self.images_directory, os.listdir(self.images_directory)[0]\n",
    "            )\n",
    "            self.mask_directory = os.path.join(\n",
    "                self.images_directory, os.listdir(self.images_directory)[1]\n",
    "            )\n",
    "\n",
    "            self.categories = os.listdir(self.base_directory)\n",
    "\n",
    "            for category in self.categories:\n",
    "                base_folder_path = os.path.join(self.base_directory, category)\n",
    "                mask_folder_path = os.path.join(self.mask_directory, category)\n",
    "\n",
    "                for image in os.listdir(base_folder_path):\n",
    "                    if image in os.listdir(mask_folder_path):\n",
    "                        self.base_images.append(\n",
    "                            self.base_transforms()(\n",
    "                                Image.fromarray(\n",
    "                                    cv2.imread(os.path.join(base_folder_path, image))\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                        self.mask_images.append(\n",
    "                            self.mask_transforms()(\n",
    "                                Image.fromarray(\n",
    "                                    cv2.imread(os.path.join(mask_folder_path, image))\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "            return self.base_images, self.mask_images\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Segmented data folder not found in the raw folder\".capitalize()\n",
    "            )\n",
    "\n",
    "    def create_dataloader(self):\n",
    "        images, masks = self.process_segmented_data()\n",
    "        data_split = train_test_split(images, masks, test_size=0.30, random_state=42)\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            dataset=list(zip(data_split[0], data_split[2])),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        val_dataloader = DataLoader(\n",
    "            dataset=list(zip(data_split[1], data_split[3])),\n",
    "            batch_size=self.batch_size * 6,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(PROCESSED_PATH):\n",
    "                dump(value=train_dataloader, filename=os.path.join(PROCESSED_PATH, \"train_dataloader.pkl\"))\n",
    "                dump(value=val_dataloader, filename=os.path.join(PROCESSED_PATH, \"val_dataloader.pkl\"))\n",
    "            else:\n",
    "                raise Exception(\"Processed data folder not found\".capitalize())\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(\"Exception caught in the section - {}\".format(e).capitalize())\n",
    "            \n",
    "    @staticmethod\n",
    "    def show_images():\n",
    "        if os.path.exists(PROCESSED_PATH):\n",
    "            val_images, val_masks = load(filename=\"../../data/processed/val_dataloader.pkl\")\n",
    "    \n",
    "            plt.figure(figsize=(30, 20))\n",
    "            \n",
    "            for index, image in enumerate(val_images):\n",
    "                plt.subplot(2 * 4, 2 * 6, 2 * index + 1)\n",
    "                image = image.permute(1, 2, 0)\n",
    "                image = (image - image.min())/(image.max() - image.min())\n",
    "                plt.imshow(image)\n",
    "                plt.title(\"Image\")\n",
    "                plt.axis(\"off\")\n",
    "                \n",
    "                plt.subplot(2*4, 2*6, 2*index + 2)\n",
    "                masks = val_masks[index].permute(1, 2, 0)\n",
    "                masks = (masks - masks.min())/(masks.max() - masks.min())\n",
    "                plt.imshow(masks, cmap=\"gray\")\n",
    "                plt.title(\"Mask\")\n",
    "                plt.axis(\"off\")\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Processed data folder not found\".capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader(image_path=\"/Users/shahmuhammadraditrahman/Desktop/cell.zip\", batch_size=4)\n",
    "loader.unzip_folder()\n",
    "loader.create_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = os.path.join(PROCESSED_PATH, \"train_dataloader.pkl\")\n",
    "val_dataloader = os.path.join(PROCESSED_PATH, \"val_dataloader.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "torch.Size([24, 3, 256, 256]) torch.Size([24, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "train_images, train_masks = next(iter(train_dataloader))\n",
    "val_images, val_masks = next(iter(val_dataloader))\n",
    "\n",
    "print(train_images.shape, train_masks.shape)\n",
    "print(val_images.shape, val_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=None, out_channels=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.model = self.encoder_block()\n",
    "\n",
    "    def encoder_block(self):\n",
    "        layers = OrderedDict()\n",
    "        layers[\"conv1\"] = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        layers[\"relu1\"] = nn.ReLU(inplace=True)\n",
    "        layers[\"conv2\"] = nn.Conv2d(\n",
    "            in_channels=self.out_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        layers[\"batch_norm1\"] = nn.BatchNorm2d(self.out_channels)\n",
    "        layers[\"relu2\"] = nn.ReLU(inplace=True)\n",
    "\n",
    "        return nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x) if x is not None else None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    encoder = Encoder(in_channels=3, out_channels=64)\n",
    "    assert encoder(torch.randn(64, 3, 256, 256)).shape == (64, 64, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels=None, out_channels=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.model = self.decoder_block()\n",
    "\n",
    "    def decoder_block(self):\n",
    "        layers = OrderedDict()\n",
    "        layers[\"deconv1\"] = nn.ConvTranspose2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "        )\n",
    "        return nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, x, skip_info):\n",
    "        if x is not None and skip_info is not None:\n",
    "            return torch.cat((self.model(x), skip_info), dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"Input and skip_info cannot be None\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    encoder = Encoder(in_channels=3, out_channels=64)\n",
    "    decoder = Decoder(in_channels=64, out_channels=64)\n",
    "\n",
    "    skip_info = encoder(torch.randn(64, 3, 256, 256))\n",
    "    noise_samples = torch.randn(64, 64, 128, 128)\n",
    "\n",
    "    assert decoder(noise_samples, skip_info).shape == (64, 128, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder_layer1 = Encoder(in_channels=3, out_channels=64)\n",
    "        self.encoder_layer2 = Encoder(in_channels=64, out_channels=128)\n",
    "        self.encoder_layer3 = Encoder(in_channels=128, out_channels=256)\n",
    "        self.encoder_layer4 = Encoder(in_channels=256, out_channels=512)\n",
    "        self.bottom_layer = Encoder(in_channels=512, out_channels=1024)\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.intermediate_layer1 = Encoder(in_channels=1024, out_channels=512)\n",
    "        self.intermediate_layer2 = Encoder(in_channels=512, out_channels=256)\n",
    "        self.intermediate_layer3 = Encoder(in_channels=256, out_channels=128)\n",
    "        self.intermediate_layer4 = Encoder(in_channels=128, out_channels=64)\n",
    "\n",
    "        self.decoder_layer1 = Decoder(in_channels=1024, out_channels=512)\n",
    "        self.decoder_layer2 = Decoder(in_channels=512, out_channels=256)\n",
    "        self.decoder_layer3 = Decoder(in_channels=256, out_channels=128)\n",
    "        self.decoder_layer4 = Decoder(in_channels=128, out_channels=64)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        enc1_out = self.encoder_layer1(x)\n",
    "        pooled_enc1 = self.max_pool(enc1_out)\n",
    "\n",
    "        enc2_out = self.encoder_layer2(pooled_enc1)\n",
    "        pooled_enc2 = self.max_pool(enc2_out)\n",
    "\n",
    "        enc3_out = self.encoder_layer3(pooled_enc2)\n",
    "        pooled_enc3 = self.max_pool(enc3_out)\n",
    "\n",
    "        enc4_out = self.encoder_layer4(pooled_enc3)\n",
    "        pooled_enc4 = self.max_pool(enc4_out)\n",
    "\n",
    "        bottom_out = self.bottom_layer(pooled_enc4)\n",
    "\n",
    "        # Decoder path\n",
    "        dec1_input = self.decoder_layer1(bottom_out, enc4_out)\n",
    "        dec1_out = self.intermediate_layer1(dec1_input)\n",
    "\n",
    "        dec2_input = self.decoder_layer2(dec1_out, enc3_out)\n",
    "        dec2_out = self.intermediate_layer2(dec2_input)\n",
    "\n",
    "        dec3_input = self.decoder_layer3(dec2_out, enc2_out)\n",
    "        dec3_out = self.intermediate_layer3(dec3_input)\n",
    "\n",
    "        dec4_input = self.decoder_layer4(dec3_out, enc1_out)\n",
    "        dec4_out = self.intermediate_layer4(dec4_input)\n",
    "\n",
    "        # Final output\n",
    "        final_output = self.final_layer(dec4_out)\n",
    "\n",
    "        return final_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
